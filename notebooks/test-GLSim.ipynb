{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bc34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"submodules\", \"GLSim\"))\n",
    "\n",
    "import torch\n",
    "\n",
    "#! pip install einops\n",
    "#! pip install ml-collections\n",
    "from glsim.model_utils import ViTGLSim, ViTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5356952",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vit_b16'\n",
    "cfg = ViTConfig(model_name, debugging=True, classifier='cls', dynamic_anchor=True,\n",
    "    reducer='cls', aggregator=True, aggregator_norm=True, aggregator_num_hidden_layers=1)\n",
    "model = ViTGLSim(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb472f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenizing:  torch.Size([2, 3, 224, 224])\n",
      "After tokenizing:  torch.Size([2, 197, 768])\n",
      "Before encoder:  torch.Size([2, 197, 768])\n",
      "After encoder:  torch.Size([2, 197, 768])\n",
      "Obtaining crops:  torch.Size([2, 197, 768]) torch.Size([2, 3, 224, 224])\n",
      "Distances and 1-d indexes:  torch.Size([2, 196]) torch.Size([2, 2]) torch.Size([2, 2]) tensor([0.8908, 0.8895]) tensor([184,  90])\n",
      "2d indexes:  tensor([13,  6]) tensor([2, 6]) tensor([96, 80]) tensor([208, 112]) tensor([ 32, 176]) tensor([ 96, 208])\n",
      "\n",
      "Processing crops:  torch.Size([2, 197, 768]) torch.Size([2, 3, 224, 224])\n",
      "Before tokenizing:  torch.Size([2, 3, 224, 224])\n",
      "After tokenizing:  torch.Size([2, 197, 768])\n",
      "Before encoder:  torch.Size([2, 197, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hcchua/FGR-PTSSM/submodules/GLSim/glsim/model_utils/glsim.py:182: UserWarning: `nn.functional.upsample_bilinear` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  crop = F.upsample_bilinear(crop, size=(images.shape[-1], images.shape[-1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoder:  torch.Size([2, 197, 768])\n",
      "After concatenating:  torch.Size([2, 394, 768])\n",
      "\n",
      "After reducer:  torch.Size([2, 2, 768])\n",
      "After aggregator:  torch.Size([2, 2, 768])\n",
      "After classifier head:  torch.Size([2, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[[[0.1059, 0.1886, 0.2713,  ..., 0.8358, 0.8203, 0.8048],\n",
       "           [0.1979, 0.2191, 0.2404,  ..., 0.5815, 0.6639, 0.7463],\n",
       "           [0.2898, 0.2497, 0.2095,  ..., 0.3272, 0.5075, 0.6878],\n",
       "           ...,\n",
       "           [0.9754, 0.8566, 0.7378,  ..., 0.4664, 0.3250, 0.1836],\n",
       "           [0.5046, 0.4726, 0.4406,  ..., 0.5731, 0.5267, 0.4803],\n",
       "           [0.0337, 0.0885, 0.1433,  ..., 0.6798, 0.7285, 0.7771]],\n",
       " \n",
       "          [[0.4957, 0.6129, 0.7301,  ..., 0.9444, 0.9649, 0.9853],\n",
       "           [0.3888, 0.4432, 0.4977,  ..., 0.6631, 0.7870, 0.9110],\n",
       "           [0.2819, 0.2736, 0.2653,  ..., 0.3817, 0.6092, 0.8368],\n",
       "           ...,\n",
       "           [0.1241, 0.3571, 0.5901,  ..., 0.3748, 0.4893, 0.6038],\n",
       "           [0.3749, 0.4478, 0.5207,  ..., 0.5460, 0.6506, 0.7552],\n",
       "           [0.6258, 0.5385, 0.4512,  ..., 0.7172, 0.8119, 0.9065]],\n",
       " \n",
       "          [[0.4762, 0.5641, 0.6519,  ..., 0.4116, 0.3286, 0.2456],\n",
       "           [0.5353, 0.5540, 0.5727,  ..., 0.4450, 0.4470, 0.4490],\n",
       "           [0.5944, 0.5440, 0.4935,  ..., 0.4784, 0.5654, 0.6524],\n",
       "           ...,\n",
       "           [0.0139, 0.2474, 0.4809,  ..., 0.2173, 0.3024, 0.3875],\n",
       "           [0.2251, 0.3213, 0.4174,  ..., 0.3563, 0.4437, 0.5311],\n",
       "           [0.4363, 0.3952, 0.3540,  ..., 0.4953, 0.5850, 0.6747]]],\n",
       " \n",
       " \n",
       "         [[[0.0099, 0.0828, 0.1557,  ..., 0.5190, 0.5982, 0.6774],\n",
       "           [0.1378, 0.2016, 0.2655,  ..., 0.5841, 0.6525, 0.7209],\n",
       "           [0.2656, 0.3204, 0.3752,  ..., 0.6491, 0.7067, 0.7643],\n",
       "           ...,\n",
       "           [0.3655, 0.4182, 0.4709,  ..., 0.4623, 0.4640, 0.4658],\n",
       "           [0.4224, 0.4630, 0.5035,  ..., 0.5062, 0.5237, 0.5412],\n",
       "           [0.4792, 0.5077, 0.5362,  ..., 0.5501, 0.5834, 0.6167]],\n",
       " \n",
       "          [[0.7822, 0.7777, 0.7731,  ..., 0.2527, 0.2386, 0.2244],\n",
       "           [0.7569, 0.7453, 0.7337,  ..., 0.2511, 0.2428, 0.2346],\n",
       "           [0.7316, 0.7130, 0.6944,  ..., 0.2495, 0.2471, 0.2447],\n",
       "           ...,\n",
       "           [0.2722, 0.3281, 0.3841,  ..., 0.7659, 0.8666, 0.9673],\n",
       "           [0.2318, 0.3041, 0.3764,  ..., 0.7811, 0.8801, 0.9790],\n",
       "           [0.1914, 0.2800, 0.3687,  ..., 0.7963, 0.8935, 0.9908]],\n",
       " \n",
       "          [[0.1529, 0.2252, 0.2976,  ..., 0.8195, 0.8751, 0.9307],\n",
       "           [0.1337, 0.2052, 0.2767,  ..., 0.7297, 0.7814, 0.8331],\n",
       "           [0.1145, 0.1851, 0.2557,  ..., 0.6399, 0.6877, 0.7355],\n",
       "           ...,\n",
       "           [0.5128, 0.5293, 0.5458,  ..., 0.5492, 0.5120, 0.4749],\n",
       "           [0.4507, 0.4901, 0.5296,  ..., 0.5301, 0.4691, 0.4081],\n",
       "           [0.3886, 0.4510, 0.5134,  ..., 0.5110, 0.4261, 0.3413]]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, cfg.num_channels, cfg.image_size, cfg.image_size)\n",
    "out = model(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054efe64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGR-PTSSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from timm.models.vision_transformer import vit_base_patch16_224\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from src.pim_module_2 import PluginMoodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = vit_base_patch16_224(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = {\n",
    "    \"blocks.8\": \"layer1\",\n",
    "    \"blocks.9\": \"layer2\",\n",
    "    \"blocks.10\": \"layer3\",\n",
    "    \"blocks.11\": \"layer4\",\n",
    "}\n",
    "num_selects = {\n",
    "    \"layer1\": 32,\n",
    "    \"layer2\": 32,\n",
    "    \"layer3\": 32,\n",
    "    \"layer4\": 32,\n",
    "}\n",
    "model = PluginMoodel(\n",
    "    backbone=backbone,\n",
    "    return_nodes=return_nodes,\n",
    "    img_size=224,\n",
    "    use_fpn=True,\n",
    "    fpn_size=256,\n",
    "    proj_type=\"Linear\",\n",
    "    upsample_type=\"Conv\",\n",
    "    use_selection=True,\n",
    "    num_classes=10,\n",
    "    num_selects=num_selects,\n",
    "    use_combiner=True,\n",
    "    comb_proj_size=None,\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(10, 3, 224, 224)\n",
    "    output = model(x)\n",
    "    #print(output)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "model = vit_base_patch16_224(init_values=1e-5)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input = torch.randn(1, 3, 224, 224)\n",
    "    output = model(input)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nodes, eval_nodes = get_graph_node_names(model)\n",
    "# train_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nodes in the `train_nodes` and `eval_nodes` lists represent the different layers and operations in the `Model` class.\n",
    "# Here's a breakdown of what each node corresponds to:\n",
    "#\n",
    "# 1. **'x'**: The input tensor to the model.\n",
    "# 2. **'conv1.0'**: The first convolutional layer in `conv1` (Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))).\n",
    "# 3. **'conv1.1'**: The first batch normalization layer in `conv1` (BatchNorm2d(64)).\n",
    "# 4. **'conv1.2'**: The first ReLU activation in `conv1`.\n",
    "# 5. **'conv1.3'**: The second convolutional layer in `conv1` (Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))).\n",
    "# 6. **'conv1.4'**: The second batch normalization layer in `conv1` (BatchNorm2d(64)).\n",
    "# 7. **'conv1.5'**: The second ReLU activation in `conv1`.\n",
    "# 8. **'conv2.0'**: The first convolutional layer in `conv2` (Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))).\n",
    "# 9. **'conv2.1'**: The first batch normalization layer in `conv2` (BatchNorm2d(128)).\n",
    "# 10. **'conv2.2'**: The first ReLU activation in `conv2`.\n",
    "# 11. **'conv2.3'**: The second convolutional layer in `conv2` (Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))).\n",
    "# 12. **'conv2.4'**: The second batch normalization layer in `conv2` (BatchNorm2d(128)).\n",
    "# 13. **'conv2.5'**: The second ReLU activation in `conv2`.\n",
    "# 14. **'pool'**: The adaptive average pooling layer (AdaptiveAvgPool2d(output_size=(1, 1))).\n",
    "# 15. **'flatten'**: The operation that flattens the tensor before passing it to the classifier.\n",
    "# 16. **'classifier'**: The final linear layer that outputs the class scores (Linear(in_features=128, out_features=10, bias=True)).\n",
    "\n",
    "# These nodes represent the sequence of operations that the input tensor goes through as it passes through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGR-PTSSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
